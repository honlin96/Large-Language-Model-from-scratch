{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f6f0d",
   "metadata": {},
   "source": [
    "This notebook heavily refers to the following resources:\n",
    "1. https://www.youtube.com/watch?v=viZrOnJclY0&ab_channel=StatQuestwithJoshStarmer\n",
    "\n",
    "Word embedding is the process of converting a word into a vector that can be processed by a neural network. Unlike a tokenizer, where the input is simply being converted to a random token, the embedding process aims to create an embedded dictionary where words of similar meaning will be ``closer\" when embedded into a vector. Words may also carry different meaning, therefore we hope to assign more than 1 number to describe different words. In this notebook, we are going to train our own word embedder using neural network. \n",
    "\n",
    "Consider the following training data\n",
    "1. Fruits are tasty.\n",
    "2. Cakes are tasty.\n",
    "\n",
    "In our training data, there are a total of 4 unique words: fruits, are, tasty, cakes. Using the one hot encoding method, we can assign each unique words with a one vector, and the output of the word with another vector. \n",
    "\n",
    "Input:\n",
    "1. fruits - [1, 0, 0, 0]\n",
    "2. are    - [0, 1, 0, 0]\n",
    "3. tasty  - [0, 0, 1, 0]\n",
    "4. cakes  - [0, 0, 0, 1]\n",
    "\n",
    "Label:\n",
    "[0, 1, 0, 0]\n",
    "[0, 0, 1, 0]\n",
    "[0, 0, 0, 1]\n",
    "[0, 1, 0, 0]\n",
    "\n",
    "Consider the following 3 training data:\n",
    "1. Fruits are tasty.\n",
    "2. Fruits are delicious.\n",
    "3. Fruits are tasty and delicious\n",
    "\n",
    "The word 'are' has 2 possible outputs, tasty and delicious. One hot encoding is unable to encode this relation properly. This becomes a multi-label encoding scheme, where each sample can be associated with multiple binary labels, with each label indicating the presence or absence of a category. \n",
    "\n",
    "Label :\n",
    "[0, 1, 0, 0, 0]\n",
    "[0, 0, 1, 1, 0]\n",
    "[0, 0, 0, 0, 1]\n",
    "[0, 0, 0, 0, 1]\n",
    "[0, 0, 1, 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In pytorch, this is done through the torch.tensor() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba72c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor([[1., 0., 0., 0.],\n",
    "                       [0., 1., 0., 0.],\n",
    "                       [0., 0., 1., 0.],\n",
    "                       [0., 0., 0., 1.]])\n",
    "\n",
    "labels = torch.tensor([[0., 1., 0., 0.],\n",
    "                       [0., 0., 1., 0.],\n",
    "                       [0., 0., 0., 1.],\n",
    "                       [0., 1., 0., 0.]])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc37e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2859970609.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class wordembedding():\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class wordembedding():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856834b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_nwq",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
