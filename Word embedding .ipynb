{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f6f0d",
   "metadata": {},
   "source": [
    "This notebook heavily refers to the following resources:\n",
    "1. https://www.youtube.com/watch?v=viZrOnJclY0&ab_channel=StatQuestwithJoshStarmer\n",
    "2. https://www.youtube.com/watch?v=Qf06XDYXCXI&t=1070s&ab_channel=StatQuestwithJoshStarmer\n",
    "\n",
    "Word embedding is the process of converting a word into a vector that can be processed by a neural network. Unlike a tokenizer, where the input is simply being converted to a random token, the embedding process aims to create an embedded dictionary where words of similar meaning will be ``closer\" when embedded into a vector. Words may also carry different meaning, therefore we hope to assign more than 1 number to describe different words. In this notebook, we are going to train our own word embedder using neural network. \n",
    "\n",
    "Consider the following training data\n",
    "1. Fruits are tasty.\n",
    "2. Cakes are tasty.\n",
    "\n",
    "In our training data, there are a total of 4 unique words: fruits, are, tasty, cakes. Using the one hot encoding method, we can assign each unique words with a one vector, and the output of the word with another vector. \n",
    "\n",
    "Input:\n",
    "1. fruits - [1, 0, 0, 0]\n",
    "2. are    - [0, 1, 0, 0]\n",
    "3. tasty  - [0, 0, 1, 0]\n",
    "4. cakes  - [0, 0, 0, 1]\n",
    "\n",
    "Label:\n",
    "[0, 1, 0, 0]\n",
    "[0, 0, 1, 0]\n",
    "[0, 0, 0, 1]\n",
    "[0, 1, 0, 0]\n",
    "\n",
    "Consider the following 3 training data:\n",
    "1. Fruits are tasty.\n",
    "2. Fruits are delicious.\n",
    "3. Fruits are tasty and delicious\n",
    "\n",
    "The word 'are' has 2 possible outputs, tasty and delicious. One hot encoding is unable to encode this relation properly. This becomes a multi-label encoding scheme, where each sample can be associated with multiple binary labels, with each label indicating the presence or absence of a category. \n",
    "\n",
    "Label :\n",
    "[0, 1, 0, 0, 0]\n",
    "[0, 0, 1, 1, 0]\n",
    "[0, 0, 0, 0, 1]\n",
    "[0, 0, 0, 0, 1]\n",
    "[0, 0, 1, 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In pytorch, this is done through the torch.tensor() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba72c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.uniform import Uniform\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = torch.tensor([[1., 0., 0., 0.],\n",
    "                       [0., 1., 0., 0.],\n",
    "                       [0., 0., 1., 0.],\n",
    "                       [0., 0., 0., 1.]])\n",
    "\n",
    "labels = torch.tensor([[0., 1., 0., 0.],\n",
    "                       [0., 0., 1., 0.],\n",
    "                       [0., 0., 0., 1.],\n",
    "                       [0., 1., 0., 0.]])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc37e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__() #this is for lightning module\n",
    "        min_value = -0.5\n",
    "        max_value = 0.5\n",
    "\n",
    "        #initialize the weight for network with 2 nodes\n",
    "        self.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        #define loss function\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        #one-hot encodding\n",
    "        #print(input)\n",
    "        input = input[0] #make it into a vector instead of a tensor\n",
    "        inputs_to_1st_hidden = ((input[0] * self.input1_w1) + \n",
    "                                (input[1] * self.input2_w1) +\n",
    "                                (input[2] * self.input3_w1) +\n",
    "                                (input[3] * self.input4_w1))\n",
    "        inputs_to_2nd_hidden = ((input[0] * self.input1_w1) +\n",
    "                                (input[1] * self.input2_w2) +\n",
    "                                (input[2] * self.input3_w2) +\n",
    "                                (input[3] * self.input4_w2))\n",
    "        output1 = ((inputs_to_1st_hidden * self.output1_w1) +\n",
    "                   (inputs_to_2nd_hidden *self.output1_w2))\n",
    "        output2 = ((inputs_to_1st_hidden * self.output2_w1) +\n",
    "                   (inputs_to_2nd_hidden *self.output2_w2))\n",
    "        output3 = ((inputs_to_1st_hidden * self.output3_w1) +\n",
    "                   (inputs_to_2nd_hidden *self.output3_w2))\n",
    "        output4 = ((inputs_to_1st_hidden * self.output4_w1) +\n",
    "                   (inputs_to_2nd_hidden *self.output4_w2))\n",
    "\n",
    "        output_presoftmax = torch.stack([output1, output2, output3, output4])\n",
    "        return(output_presoftmax)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr = 0.1)\n",
    "\n",
    "    def training_step(self,batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = self.loss(output_i, label_i[0])\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856834b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before parameterization, the parameters are :\n",
      "input1_w1 tensor(-0.0614)\n",
      "input1_w2 tensor(-0.1802)\n",
      "input2_w1 tensor(0.0847)\n",
      "input2_w2 tensor(-0.4365)\n",
      "input3_w1 tensor(0.3641)\n",
      "input3_w2 tensor(0.4052)\n",
      "input4_w1 tensor(-0.4422)\n",
      "input4_w2 tensor(0.1040)\n",
      "output1_w1 tensor(0.1997)\n",
      "output1_w2 tensor(-0.1207)\n",
      "output2_w1 tensor(0.4642)\n",
      "output2_w2 tensor(0.4567)\n",
      "output3_w1 tensor(-0.2053)\n",
      "output3_w2 tensor(-0.3855)\n",
      "output4_w1 tensor(0.1262)\n",
      "output4_w2 tensor(-0.4925)\n"
     ]
    }
   ],
   "source": [
    "smallmodel = WordEmbedding()\n",
    "print('Before parameterization, the parameters are :')\n",
    "for name, param in smallmodel.named_parameters():\n",
    "    print(name, param.data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99628e3a-80d7-4c82-9af0-9b99283acabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         w1        w2   token   input\n",
      "0 -0.061420 -0.180230  Fruits  input1\n",
      "1  0.084740 -0.436456     are  input2\n",
      "2  0.364129  0.405175   tasty  input3\n",
      "3 -0.442168  0.104045   Cakes  input4\n"
     ]
    }
   ],
   "source": [
    "#graphing the data\n",
    "\n",
    "#put the weight into dictionary\n",
    "data = {'w1': [smallmodel.input1_w1.item(),\n",
    "               smallmodel.input2_w1.item(),\n",
    "               smallmodel.input3_w1.item(),\n",
    "               smallmodel.input4_w1.item()],\n",
    "        'w2': [smallmodel.input1_w2.item(),\n",
    "               smallmodel.input2_w2.item(),\n",
    "               smallmodel.input3_w2.item(),\n",
    "               smallmodel.input4_w2.item(),],\n",
    "        'token' : [\"Fruits\", \"are\", \"tasty\", \"Cakes\"],\n",
    "        'input' : ['input1', 'input2', 'input3', 'input4']\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#sns.scatterplot(data = df, x = 'w1', y = 'w2')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eaf1e38-649a-4d8c-8685-19f330b84bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\honlin\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | loss         | CrossEntropyLoss | 0     \n",
      "  | other params | n/a              | 16    \n",
      "--------------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "C:\\Users\\honlin\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\honlin\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961dad4e76c449bbb1c32cd653e150f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "# training the neural network\n",
    "trainer = pl.Trainer(max_epochs = 100)\n",
    "trainer.fit(smallmodel, train_dataloaders = dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13c992-f76b-4242-9f9f-24e7a6bde29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         w1        w2   token   input\n",
      "0  2.254447 -0.180230  Fruits  input1\n",
      "1 -1.877416 -2.382924     are  input2\n",
      "2  2.704787 -1.146925   tasty  input3\n",
      "3 -1.164742  2.272458   Cakes  input4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#put the weight into dictionary\n",
    "data = {'w1': [smallmodel.input1_w1.item(),\n",
    "               smallmodel.input2_w1.item(),\n",
    "               smallmodel.input3_w1.item(),\n",
    "               smallmodel.input4_w1.item()],\n",
    "        'w2': [smallmodel.input1_w2.item(),\n",
    "               smallmodel.input2_w2.item(),\n",
    "               smallmodel.input3_w2.item(),\n",
    "               smallmodel.input4_w2.item(),],\n",
    "        'token' : [\"Fruits\", \"are\", \"tasty\", \"Cakes\"],\n",
    "        'input' : ['input1', 'input2', 'input3', 'input4']\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#sns.scatterplot(data = df, x = 'w1', y = 'w2')\n",
    "print(df)\n",
    "plt.figure()\n",
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6b041-3d4b-48dd-8e03-af5f2a589256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
